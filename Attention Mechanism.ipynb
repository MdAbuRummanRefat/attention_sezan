{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Implementation of Attention Mechanism for English to Bangla Translation\n",
    "MD Muhaimin Rahman\n",
    "contact: sezan92[at]gmail[dot]com\n",
    "\n",
    "In this project I have implemented -at least tried to implement- Attention Mechanism for Encoder-Decoder Deep Learning Network for English To Bangla Translation in keras. Neural Machine Translation is a case for Encoder Decoder network. An example is given in Jason Brownlee's [blog](https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/) . But this architecture had a problem for long sentences . Bahdanau et al. used Attention mechanism for Neural Machine Translation , in this [paper](https://arxiv.org/abs/1409.0473). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input,Permute,Dense,LSTM,Embedding,Bidirectional,multiply,RepeatVector,Flatten,Activation\n",
    "from keras.models import Model\n",
    "from keras.activations import softmax\n",
    "from keras.utils import plot_model\n",
    "import text_preprocess_utils_fh as tpu\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing text. I have wrote a class ```text_prep``` in the file ```text_preprocess_utils_fh.py``` . Please have a look at the file for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = tpu.text_prep('/data/ben.txt',limit=20000)\n",
    "tp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice\n",
    "The file is saved at ```data``` folder . If you want to work on your computer write ```tp = tpu.text_prep('data/ben.txt',limit=20000)```. the extra ```/``` is for floydhub cloud gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 12\n",
    "source_input,input_starter,output_encoded = tp.preprocess(max_words)\n",
    "# In[get vocabulary]\n",
    "source_words,target= tp.get_vocab()\n",
    "source_vocab = source_words[0] #first one is vocabulary list\n",
    "source_i2w = source_words[1] # second one is vocabulary index to word \n",
    "source_w2i = source_words[2] # third one is vocabulary word to index\n",
    "target_vocab = target[0] #first one is vocabulary list\n",
    "target_i2w = target[1] # second one is vocabulary index to word\n",
    "target_w2i = target[2] # third one is vocabulary word to index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab size for both languages. Notice that I have added 2 for both size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_vocab_size = len(source_vocab)+2\n",
    "target_vocab_size = len(target_vocab)+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "timesteps i.e. words per sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "source_timesteps = source_input.shape[1]\n",
    "target_timesteps = output_encoded.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM units . Notice that first one is half of second one. It's because I have used Bidirectional LSTM for first one, which doubles the given units at the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_a= 32\n",
    "n_s =64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "em_shape=100\n",
    "batch_size=64\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving the vocabulary for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(file=open('/output/ben_eng_source_vocab_fh.pkl','wb'),obj=source_words)\n",
    "pickle.dump(file=open('/output/ben_eng_target_vocab_fh.pkl','wb'),obj=target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = Input(shape=(source_timesteps,),name='source') #Source sequence\n",
    "source_emb = Embedding(input_dim=source_vocab_size ,output_dim=100,name='source_embedding',mask_zero=True)(source) #Embedding for source sequence\n",
    "h_source = Bidirectional(LSTM(32,return_sequences=True,name='h_s'))(source_emb) #Hidden state of source sequence\n",
    "initial_hidden = Input(shape=(n_s,),name='hidden_target') #Initial hidden state of target , we will give input <s> as starting of the sequence\n",
    "init_state_att=initial_hidden # Initial hidden state of the target sentence\n",
    "init_hid = Input(shape=(n_s,),name='cell_target') # Initial cell state of the target sentence\n",
    "init_hid_att=init_hid\n",
    "init_state_att_repeat = RepeatVector(source_timesteps)(init_state_att)\n",
    "output=[] #Output empty list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention!\n",
    "Now comes the real stuff. The following is pseudo code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for each word in target sentence:\n",
    "    merge = (previous_hidden_state x source_hidden_state)\n",
    "    score = tanh(merge)\n",
    "    attention_probability = softmax(score)\n",
    "    context = (attention_probability x source_hidden_state)\n",
    "    previous_hidden_state = LSTM(context)\n",
    "    next_word = softmax(previous_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(target_timesteps): # For loop for manually looping through sequences\n",
    "    merged = multiply([init_state_att_repeat,h_source]) #Dot product as of h_t and h_s\n",
    "    score = Dense(1,activation='tanh')(merged) # tanh(h_txh_s)\n",
    "    attention_prob = Dense(1,activation='softmax')(score) #prob = softmax(tanh(h_t x h_s))\n",
    "    context = multiply([h_source,attention_prob]) #context = prob x h_source\n",
    "    init_state_att,_,init_hid_att = LSTM(64,return_state=True)(context,initial_state=[init_state_att,init_hid_att]) #hidden state of next word of target\n",
    "    init_state_att_repeat = RepeatVector(source_timesteps)(init_state_att) #making it 3D by repeat vector\n",
    "    #context = merge([attention_prob,h_source],mode='mul',name='context_vector')\n",
    "    prediction = Dense(target_vocab_size,activation='softmax',kernel_regularizer=l2())(init_state_att) #predicting next word\n",
    "    output.append(prediction) #appending to output list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model =Model(inputs=[source,initial_hidden,init_hid],outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(0.008),loss='sparse_categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"/output/attention_ben_%d_words_eng.best.hdf5\"%max_words\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_file = model.to_json()\n",
    "with open('/output/ben_model_%d_words.json'%max_words,'w') as file:\n",
    "    file.write(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train1,x_test1,y_train,y_test= train_test_split(\n",
    "        source_input,\n",
    "        output_encoded)\n",
    "x_train2 = np.zeros((x_train1.shape[0],n_s))\n",
    "x_train3 = np.zeros((x_train1.shape[0],n_s))\n",
    "\n",
    "x_test2 = np.zeros((x_test1.shape[0],n_s))\n",
    "x_test3 = np.zeros((x_test1.shape[0],n_s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x=[np.array(x_train1),\n",
    "             x_train2,x_train3],\n",
    "    y=list(y_train.swapaxes(0,1)),\n",
    "    validation_data=([np.array(x_test1),\n",
    "             x_test2,x_test3],\n",
    "    list(y_test.swapaxes(0,1))),\n",
    "    batch_size=batch_size,epochs=epochs,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"/output/attention_ben_%d_words_eng.final.hdf5\"%max_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
